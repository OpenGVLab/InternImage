<!-- ## 欢迎访问OpenGVLab! 👋
 -->
<!-- [主页](https://opengvlab.github.io/) -->

<div align=center>
<img src=https://github.com/OpenGVLab/.github/blob/main/assets/opengvlab-logo.png>
</div>

---
![GitHub Org's stars](https://img.shields.io/github/stars/opengvlab?style=social)
![GitHub Org's follows](https://img.shields.io/github/followers/opengvlab?style=social)
![GitHub](https://img.shields.io/github/license/OpenGVLab/InternImage)

<!--

**Here are some ideas to get you started:**

🙋‍♀️ A short introduction - what is your organization all about?
🌈 Contribution guidelines - how can the community get involved?
👩‍💻 Useful resources - where can the community find your docs? Is there anything else the community should know?
🍿 Fun facts - what does your team eat for breakfast?
🧙 Remember, you can do mighty things with the power of [Markdown](https://docs.github.com/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)
-->


### 简介
OpenGVLab致力于通用视觉模型的开源社区建设，开源项目覆盖数据、模型、评测基准全链路，为学术界和产业界的多模态通用模型研发提供了坚实的支撑。OpenGVLab可以帮助开发者显著降低通用视觉模型的开发门槛，用更低成本快速开发用于成百上千种视觉任务、视觉场景的算法模型，高效实现对长尾场景的覆盖，推动通用AI技术的规模化应用。


### 工作

* 数据：OpenGVLab构建了千万级超大规模精标注数据集，涵盖了图像分类、目标检测等视觉核心任务的标注
* 模型：OpenGVLab的开源项目全方位覆盖了通用模型架构、高效训练框架及超高性能的预训练模型，助力社区用极低的数据量快速满足多场景、多任务、高性能的AI模型训练
* 评测基准：OpenGVLab提供了多任务、多模态的通用视觉评测基准，可以提供权威的评测结果，推动基于统一标准的公平和准确评测，加快通用视觉模型的产业化应用步伐

### 成果
* 书生1.0：解决分类、目标检测、语义分割、深度估计四大视觉核心任务的通用模型
* 书生2.0：10亿参数通用图像和视频模型
* 书生2.5：30亿参数多模态多任务通用大模型
<!-- ### Our Work

* Competition winning solutions 🔥
  * [InternVideo-Ego4D](https://github.com/OpenGVLab/ego4d-eccv2022-solutions) - SOTA in various Ego4D challenges, ECCV 2022

* INTERN 2.0
  *  [InternImage](https://github.com/OpenGVLab/InternImage)
  *  [InternVideo](https://github.com/OpenGVLab/InternVideo)
  *  [STM-Evaluation](https://github.com/OpenGVLab/STM-Evaluation)

* INTERN 1.0
  * [modelzoo](https://github.com/OpenGVLab/modelzoo)
  * [gv-benchmark](https://github.com/OpenGVLab/gv-benchmark)

### Follow us

* [Twitter](https://twitter.com/opengvlab)
* [WeChat](./opengv-wechat.jpeg) -->
