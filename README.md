# ä¹¦ç”Ÿ2.5 - å¤šæ¨¡æ€å¤šä»»åŠ¡é€šç”¨å¤§æ¨¡å‹
<div align=center>
<img src='./docs/figs/log.png' width=600>
</div>

[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-coco)](https://paperswithcode.com/sota/object-detection-on-coco?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-coco-minival)](https://paperswithcode.com/sota/object-detection-on-coco-minival?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-lvis-v1-0-minival)](https://paperswithcode.com/sota/object-detection-on-lvis-v1-0-minival?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-lvis-v1-0-val)](https://paperswithcode.com/sota/object-detection-on-lvis-v1-0-val?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-pascal-voc-2007)](https://paperswithcode.com/sota/object-detection-on-pascal-voc-2007?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-pascal-voc-2012)](https://paperswithcode.com/sota/object-detection-on-pascal-voc-2012?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-openimages-v6)](https://paperswithcode.com/sota/object-detection-on-openimages-v6?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/object-detection-on-crowdhuman-full-body)](https://paperswithcode.com/sota/object-detection-on-crowdhuman-full-body?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/2d-object-detection-on-bdd100k-val)](https://paperswithcode.com/sota/2d-object-detection-on-bdd100k-val?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/semantic-segmentation-on-ade20k)](https://paperswithcode.com/sota/semantic-segmentation-on-ade20k?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/semantic-segmentation-on-cityscapes)](https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/semantic-segmentation-on-cityscapes-val)](https://paperswithcode.com/sota/semantic-segmentation-on-cityscapes-val?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/semantic-segmentation-on-pascal-context)](https://paperswithcode.com/sota/semantic-segmentation-on-pascal-context?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/semantic-segmentation-on-coco-stuff-test)](https://paperswithcode.com/sota/semantic-segmentation-on-coco-stuff-test?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bevformer-v2-adapting-modern-image-backbones/3d-object-detection-on-nuscenes-camera-only)](https://paperswithcode.com/sota/3d-object-detection-on-nuscenes-camera-only?p=bevformer-v2-adapting-modern-image-backbones)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/image-classification-on-inaturalist-2018)](https://paperswithcode.com/sota/image-classification-on-inaturalist-2018?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/image-classification-on-places365)](https://paperswithcode.com/sota/image-classification-on-places365?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/image-classification-on-places205)](https://paperswithcode.com/sota/image-classification-on-places205?p=internimage-exploring-large-scale-vision)
[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/internimage-exploring-large-scale-vision/image-classification-on-imagenet)](https://paperswithcode.com/sota/image-classification-on-imagenet?p=internimage-exploring-large-scale-vision)

<!-- è¿™ä¸ªä»£ç ä»“åº“æ˜¯InterImageçš„å®˜æ–¹å®ç°ã€‚ [InternImage: Exploring Large-Scale Vision Foundation Models with
Deformable Convolutions](https://arxiv.org/abs/2211.05778). -->


[æ–‡ç« ](https://arxiv.org/abs/2211.05778) \| [åšå®¢](https://zhuanlan.zhihu.com/p/610772005) | [æ–‡æ¡£](./docs/)



<!-- ## ä¹¦ç”Ÿ2.5-30äº¿å‚æ•°è§†è§‰é€šç”¨ä¸»å¹²æ¨¡å‹ -->
## ç®€ä»‹
å•†æ±¤ç§‘æŠ€ä¸ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤åœ¨2023å¹´3æœˆ14æ—¥è”åˆå‘å¸ƒå¤šæ¨¡æ€å¤šä»»åŠ¡é€šç”¨å¤§æ¨¡å‹â€œä¹¦ç”Ÿ2.5â€ã€‚â€œä¹¦ç”Ÿ2.5â€åœ¨å¤šæ¨¡æ€å¤šä»»åŠ¡å¤„ç†èƒ½åŠ›ä¸­æ–©è·å¤šé¡¹å…¨æ–°çªç ´ï¼Œå…¶å“è¶Šçš„å›¾æ–‡è·¨æ¨¡æ€ä»»åŠ¡å¤„ç†èƒ½åŠ›å¯ä¸ºè‡ªåŠ¨é©¾é©¶ç­‰é€šç”¨åœºæ™¯ä»»åŠ¡æä¾›é«˜æ•ˆç²¾å‡†çš„æ„ŸçŸ¥å’Œç†è§£èƒ½åŠ›æ”¯æŒã€‚â€œä¹¦ç”Ÿ2.5â€è‡´åŠ›äºå¤šæ¨¡æ€å¤šä»»åŠ¡é€šç”¨æ¨¡å‹çš„æ„å»ºï¼Œæ—¨åœ¨æ¥æ”¶å¤„ç†å„ç§ä¸åŒæ¨¡æ€çš„è¾“å…¥ï¼Œå¹¶é‡‡ç”¨ç»Ÿä¸€çš„æ¨¡å‹æ¶æ„å’Œå‚æ•°å¤„ç†å„ç§ä¸åŒçš„ä»»åŠ¡ï¼Œä¿ƒè¿›ä¸åŒæ¨¡æ€å’Œä»»åŠ¡ä¹‹é—´åœ¨è¡¨ç¤ºå­¦ä¹ æ–¹é¢çš„åä½œï¼Œé€æ­¥å®ç°é€šç”¨äººå·¥æ™ºèƒ½é¢†åŸŸçš„èä¼šè´¯é€šã€‚

## æ¦‚è§ˆå›¾

<div align=center>
<img src='./docs/figs/intern_pipeline.png' width=700>
</div>


## äº®ç‚¹
- :thumbsup: **é«˜è¾¾30äº¿å‚æ•°çš„æœ€å¼ºè§†è§‰é€šç”¨ä¸»å¹²æ¨¡å‹**
- ğŸ† **æ–©è·ImageNetå’ŒCOCOâ€œåŒæ–™å† å†›â€**
- ğŸ† **å¼€æºæ¨¡å‹ä¸­ImageNetå‡†ç¡®åº¦æœ€é«˜**
- ğŸ† **ç‰©ä½“æ£€æµ‹æ ‡æ†æ•°æ®é›†COCO `65.4 mAP`ï¼Œå”¯ä¸€è¶…è¿‡`65.0 mAP`çš„æ¨¡å‹**

## æœ€æ–°è¿›å±•
- 2023å¹´3æœˆ14æ—¥: ğŸš€ ä¹¦ç”Ÿ2.5å‘å¸ƒï¼
- 2023å¹´2æœˆ28æ—¥: ğŸš€ InternImage è¢«CVPR 2023æ¥æ”¶!
- 2022å¹´11æœˆ18æ—¥: ğŸš€ åŸºäº InternImage-XL ä¸»å¹²ç½‘ç»œï¼Œ[BEVFormer v2](https://arxiv.org/abs/2211.10439) åœ¨nuScenesçš„çº¯è§†è§‰3Dæ£€æµ‹ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ `63.4 NDS` ï¼
- 2022å¹´11æœˆ10æ—¥: ğŸš€ InternImage-H åœ¨COCOç›®æ ‡æ£€æµ‹ä»»åŠ¡ä¸Šä»¥ `65.4 mAP` æ–©è·å† å†›ï¼Œæ˜¯å”¯ä¸€çªç ´ `65.0 mAP` çš„è¶…å¼ºç‰©ä½“æ£€æµ‹æ¨¡å‹ï¼
- 2022å¹´11æœˆ10æ—¥: ğŸš€ InternImage-H åœ¨ADE20kè¯­ä¹‰åˆ†å‰²æ•°æ®é›†ä¸Šå–å¾— `62.9 mIoU` çš„SOTAæ€§èƒ½ï¼

## ç»å…¸æ•°æ®é›†æ€§èƒ½
### 1. ImageNet
åœ¨å›¾åƒåˆ†ç±»æ ‡æ†æ•°æ®é›†ImageNetä¸Šï¼Œâ€œä¹¦ç”Ÿ2.5â€ä»…åŸºäºå…¬å¼€æ•°æ®ä¾¿è¾¾åˆ°äº† 90.1% çš„Top-1å‡†ç¡®ç‡ã€‚è¿™æ˜¯é™¤è°·æ­Œä¸å¾®è½¯ä¸¤ä¸ªæœªå…¬å¼€æ¨¡å‹åŠé¢å¤–æ•°æ®é›†å¤–ï¼Œå”¯ä¸€å‡†ç¡®ç‡è¶…è¿‡90.0%çš„æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸–ç•Œä¸Šå¼€æºæ¨¡å‹ä¸­ImageNetå‡†ç¡®åº¦æœ€é«˜ï¼Œè§„æ¨¡æœ€å¤§çš„æ¨¡å‹ã€‚

<div align=center>
<img src='./docs/figs/imagenet_leaderboard.png' width=600>
</div>

### 2. COCO
åœ¨ç‰©ä½“æ£€æµ‹æ ‡æ†æ•°æ®é›†COCOä¸Šï¼Œâ€œä¹¦ç”Ÿ2.5â€ å–å¾—äº† 65.4 çš„ mAPï¼Œæ˜¯ä¸–ç•Œä¸Šå”¯ä¸€è¶…è¿‡65.0mAPçš„æ¨¡å‹ï¼Œå·²ç»é¢†å…ˆè°·æ­Œå’Œå¾®è½¯ã€‚
<div align=center>
<img src='./docs/figs/coco_leaderboard.png' width=600>
</div>

## å›¾æ–‡è·¨æ¨¡æ€åº”ç”¨
å›¾æ–‡è·¨æ¨¡æ€æ˜¯é€šç”¨äººå·¥æ™ºèƒ½çš„é‡è¦æ–¹å‘ï¼Œå¯ä»¥ç»“åˆå›¾åƒã€æ–‡æœ¬ç­‰ä¿¡æ¯æ¥å®ç°å¯¹ä¸–ç•Œçš„ç†è§£å’Œè®¤çŸ¥ï¼Œåœ¨è‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸå…·æœ‰é‡è¦ä¸”å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚â€œä¹¦ç”Ÿ2.5â€å¤šæ¨¡æ€å¤šä»»åŠ¡é€šç”¨æ¨¡å‹å½“å‰ä¸“æ³¨äºå›¾æ–‡è·¨æ¨¡æ€ä»»åŠ¡ï¼Œæ¨¡å‹æ— éœ€å¾®è°ƒï¼Œä¾¿å¯çµæ´»ã€ç²¾å‡†åœ°åº”å¯¹å„å¼ä»»åŠ¡ï¼Œå¹¶åœ¨â€œå›¾æ–‡æ£€ç´¢ã€ä»¥å›¾ç”Ÿæ–‡å’Œä»¥æ–‡ç”Ÿå›¾å’Œâ€ç­‰å›¾æ–‡è·¨æ¨¡æ€ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†ä»¤äººå°è±¡æ·±åˆ»çš„è¡¨ç°ã€‚åœ¨20ä½™ä¸ªä¸åŒåœºæ™¯ä¸åŒä»»åŠ¡çš„å•æ¨¡æ€å’Œè·¨æ¨¡æ€å…¬å¼€æ•°æ®é›†ï¼Œ â€œä¹¦ç”Ÿ2.5â€éƒ½å–å¾—äº†æœ€ä½³æˆç»©ã€‚
<div align=center>
<img src='./docs/figs/multi_task_sota.png' width=600>
</div>


### 1. å›¾æ–‡æ£€ç´¢

â€œä¹¦ç”Ÿ2.5â€å¯æ ¹æ®æ–‡æœ¬å†…å®¹éœ€æ±‚å¿«é€Ÿå®šä½æ£€ç´¢å‡ºè¯­ä¹‰æœ€ç›¸å…³çš„å›¾åƒã€‚è¿™ä¸€èƒ½åŠ›æ—¢å¯åº”ç”¨äºè§†é¢‘å’Œå›¾åƒé›†åˆï¼Œä¹Ÿå¯è¿›ä¸€æ­¥ç»“åˆç‰©ä½“æ£€æµ‹æ¡†ï¼Œå…·æœ‰ä¸°å¯Œçš„åº”ç”¨æ¨¡å¼ï¼Œå¸®åŠ©ç”¨æˆ·æ›´ä¾¿æ·ã€å¿«é€Ÿåœ°æ‰¾åˆ°æ‰€éœ€å›¾åƒèµ„æº, ä¾‹å¦‚å¯åœ¨ç›¸å†Œä¸­è¿”å›æ–‡æœ¬æ‰€æŒ‡å®šçš„ç›¸å…³å›¾åƒã€‚

<div align=center>
<img src='./docs/figs/image_text_retrieval.png' width=600>
</div>

### 2. ä»¥å›¾ç”Ÿæ–‡

â€œä¹¦ç”Ÿ2.5â€çš„â€œä»¥å›¾ç”Ÿæ–‡â€åœ¨å›¾åƒæè¿°ã€è§†è§‰é—®ç­”ã€è§†è§‰æ¨ç†å’Œæ–‡å­—è¯†åˆ«ç­‰å¤šä¸ªæ–¹é¢å‡æ‹¥æœ‰å¼ºå¤§çš„ç†è§£èƒ½åŠ›ã€‚ä¾‹å¦‚åœ¨è‡ªåŠ¨é©¾é©¶åœºæ™¯ä¸‹ï¼Œå¯ä»¥æå‡åœºæ™¯æ„ŸçŸ¥ç†è§£èƒ½åŠ›ï¼Œè¾…åŠ©è½¦è¾†åˆ¤æ–­äº¤é€šä¿¡å·ç¯çŠ¶æ€ã€é“è·¯æ ‡å¿—ç‰Œç­‰ä¿¡æ¯ï¼Œä¸ºè½¦è¾†çš„å†³ç­–è§„åˆ’æä¾›æœ‰æ•ˆçš„æ„ŸçŸ¥ä¿¡æ¯æ”¯æŒã€‚

<div align=center>
<img src='./docs/figs/img2text.png' width=600>
</div>

### 3. ä»¥æ–‡ç”Ÿå›¾
â€œä¹¦ç”Ÿ2.5â€çš„â€œä»¥æ–‡ç”Ÿå›¾â€èƒ½åŠ›ï¼Œå¯æ ¹æ®ç”¨æˆ·æå‡ºçš„æ–‡æœ¬åˆ›ä½œéœ€æ±‚ï¼Œåˆ©ç”¨æ‰©æ•£æ¨¡å‹ç”Ÿæˆç®—æ³•ï¼Œç”Ÿæˆæ»¡è¶³éœ€æ±‚çš„é«˜è´¨é‡ã€è‡ªç„¶çš„å†™å®å›¾åƒã€‚ä¾‹å¦‚ï¼Œé’ˆå¯¹è‡ªåŠ¨é©¾é©¶ç­‰æ•°æ®ä¾èµ–åœºæ™¯ï¼Œâ€œä¹¦ç”Ÿ2.5â€å¯ä»¥ç”Ÿæˆå„ç±»çœŸå®çš„é“è·¯äº¤é€šåœºæ™¯ï¼Œå¦‚ç¹å¿™çš„åŸå¸‚è¡—é“ã€é›¨å¤©æ‹¥æŒ¤çš„è½¦é“ã€é©¬è·¯ä¸Šå¥”è·‘çš„ç‹—ç­‰ï¼Œä»è€Œè¾…åŠ©è‡ªåŠ¨é©¾é©¶ç³»ç»Ÿçš„ç ”å‘ï¼Œä¸æ–­æé«˜Corner Caseåœºæ™¯çš„æ„ŸçŸ¥èƒ½åŠ›ä¸Šé™ã€‚
<div align=center>
<img src='./docs/figs/text2img.png' width=600>
</div>



## æ ¸å¿ƒæŠ€æœ¯
â€œä¹¦ç”Ÿ2.5â€åœ¨å›¾æ–‡è·¨æ¨¡æ€é¢†åŸŸå“è¶Šçš„æ€§èƒ½è¡¨ç°ï¼Œæºè‡ªäºåœ¨å¤šæ¨¡æ€å¤šä»»åŠ¡é€šç”¨æ¨¡å‹æŠ€æœ¯æ ¸å¿ƒæ–¹é¢çš„å¤šé¡¹åˆ›æ–°ï¼Œå®ç°äº†è§†è§‰æ ¸å¿ƒè§†è§‰æ„ŸçŸ¥å¤§æ¨¡å‹ä¸»å¹²ç½‘ç»œï¼ˆInternImageï¼‰ã€ç”¨äºæ–‡æœ¬æ ¸å¿ƒçš„è¶…å¤§è§„æ¨¡æ–‡æœ¬é¢„è®­ç»ƒç½‘ç»œï¼ˆLLMï¼‰å’Œç”¨äºå¤šä»»åŠ¡çš„å…¼å®¹è§£ç å»ºæ¨¡ï¼ˆUni-Perceiverï¼‰çš„åˆ›æ–°ç»„åˆã€‚  è§†è§‰ä¸»å¹²ç½‘ç»œInternImageå‚æ•°é‡é«˜è¾¾30äº¿ï¼Œèƒ½å¤ŸåŸºäºåŠ¨æ€ç¨€ç–å·ç§¯ç®—å­è‡ªé€‚åº”åœ°è°ƒæ•´å·ç§¯çš„ä½ç½®å’Œç»„åˆæ–¹å¼ï¼Œä»è€Œä¸ºå¤šåŠŸèƒ½è§†è§‰æ„ŸçŸ¥æä¾›å¼ºå¤§çš„è¡¨ç¤ºã€‚Uni-Perceiveré€šæ‰ä»»åŠ¡è§£ç å»ºæ¨¡é€šè¿‡å°†ä¸åŒæ¨¡æ€çš„æ•°æ®ç¼–ç åˆ°ç»Ÿä¸€çš„è¡¨ç¤ºç©ºé—´ï¼Œå¹¶å°†ä¸åŒä»»åŠ¡ç»Ÿä¸€ä¸ºç›¸åŒçš„ä»»åŠ¡èŒƒå¼ï¼Œä»è€Œèƒ½å¤Ÿä»¥ç›¸åŒçš„ä»»åŠ¡æ¶æ„å’Œå…±äº«çš„æ¨¡å‹å‚æ•°åŒæ—¶å¤„ç†å„ç§æ¨¡æ€å’Œä»»åŠ¡ã€‚


<div align=center>
<img src='./docs/figs/network.png' width=600>
</div>


## é¡¹ç›®åŠŸèƒ½
- [Coming] InternImage-H(1B)/G(3B)
- [Coming] å„ç±»downstream tasks
- ğŸš€ TensorRT æ¨ç†
- ğŸš€ InternImageç³»åˆ—åˆ†ç±»ä»£ç 
- ğŸš€ InternImage-T/S/B/L/XL ImageNet-1k é¢„è®­ç»ƒæ¨¡å‹
- ğŸš€ InternImage-L/XL ImageNet-22k é¢„è®­ç»ƒæ¨¡å‹
- ğŸš€ InternImage-T/S/B/L/XL æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²æ¨¡å‹
- ğŸš€ InternImage-T/S/B/L/XLè¯­ä¹‰åˆ†å‰²æ¨¡å‹


## å¼€æºæ¨¡å‹
- ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²: [COCO](detection/configs/mask_rcnn/)
- è¯­ä¹‰åˆ†å‰²: [ADE20K](segmentation/configs/ade20k/), [Cityscapes](segmentation/configs/cityscapes/)



## ç»å…¸è§†è§‰ä»»åŠ¡æ€§èƒ½

**ImageNetå›¾åƒåˆ†ç±»**
|      name      |   pretrain   | resolution | acc@1 | #param | FLOPs |      22K model      |      1K model       |
| :------------: | :----------: | :--------: | :---: | :-----: | :---: | :-----------------: | :-----------------: |
| InternImage-T  | ImageNet-1K  |  224x224   | 83.5  |   30M   |  5G   |          -          | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/cls_model/internimage_t_1k_224.pth) \| [cfg](classification/configs/internimage_t_1k_224.yaml) |
| InternImage-S  | ImageNet-1K  |  224x224   | 84.2  |   50M   |  8G   |          -          | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/cls_model/internimage_s_1k_224.pth) \| [cfg](classification/configs/internimage_s_1k_224.yaml) |
| InternImage-B  | ImageNet-1K  |  224x224   | 84.9  |   97M   |  16G  |          -          | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/cls_model/internimage_b_1k_224.pth) \| [cfg](classification/configs/internimage_b_1k_224.yaml) |
| InternImage-L  | ImageNet-22K |  384x384   | 87.7  |  223M   | 108G  | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/cls_model/internimage_l_22k_192to384.pth)            | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/cls_model/internimage_l_22kto1k_384.pth) \| [cfg](classification/configs/internimage_l_22kto1k_384.yaml) |
| InternImage-XL | ImageNet-22K |  384x384   | 88.0  |  335M   | 163G  | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/cls_model/internimage_xl_22k_192to384.pth)            | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/cls_model/internimage_xl_22kto1k_384.pth) \| [cfg](classification/configs/internimage_xl_22kto1k_384.yaml) |
| InternImage-H | ImageNet-22K |  224x224   | 88.9  |  1.08B   | 188G  | TBD         | TBD |
| InternImage-H | ImageNet-22K |  640x640   | 89.6  |  1.08B   | 1478G  |TBD           | TBD |
| InternImage-G | ImageNet-22K |  640x640   | 90.1  |  3B   | -  |  TBD      | TBD|

**COCOç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²**

|    backbone    |       method       | schd | box mAP | mask mAP | #param | FLOPs | Download | 
| :------------: | :----------------: | :---------: | :-----: | :------: | :-----: | :---: | :---: | 
| InternImage-T  |     Mask R-CNN     |     1x      |  47.2   |   42.5   |   49M   | 270G  |  [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/det_model/mask_rcnn_internimage_t_fpn_1x_coco.pth) \| [cfg](detection/configs/mask_rcnn/mask_rcnn_internimage_t_fpn_1x_coco.py) |
| InternImage-T  |     Mask R-CNN     |     3x      |  49.1   |   43.7   |   49M   | 270G  |  [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/det_model/mask_rcnn_internimage_t_fpn_3x_coco.pth) \| [cfg](detection/configs/mask_rcnn/mask_rcnn_internimage_t_fpn_3x_coco.py) |
| InternImage-S  |     Mask R-CNN     |     1x      |  47.8   |   43.3   |   69M   | 340G  |  [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/det_model/mask_rcnn_internimage_s_fpn_1x_coco.pth) \| [cfg](detection/configs/mask_rcnn/mask_rcnn_internimage_s_fpn_1x_coco.py) |
| InternImage-S  |     Mask R-CNN     |     3x      |  49.7   |   44.5   |   69M   | 340G  |  [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/det_model/mask_rcnn_internimage_s_fpn_3x_coco.pth) \| [cfg](detection/configs/mask_rcnn/mask_rcnn_internimage_s_fpn_3x_coco.py) |
| InternImage-B  |     Mask R-CNN     |     1x      |  48.8   |   44.0   |  115M   | 501G  |  [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/det_model/mask_rcnn_internimage_b_fpn_1x_coco.pth) \| [cfg](detection/configs/mask_rcnn/mask_rcnn_internimage_b_fpn_1x_coco.py) |
| InternImage-B  |     Mask R-CNN     |     3x      |  50.3   |   44.8   |  115M   | 501G  |  [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/det_model/mask_rcnn_internimage_b_fpn_3x_coco.pth) \| [cfg](detection/configs/mask_rcnn/mask_rcnn_internimage_b_fpn_3x_coco.py) |
| InternImage-L  |     Cascade        |     1x      |  54.9   |   47.7   |  277M   | 1399G |  [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/det_model/cascade_internimage_l_fpn_1x_coco.pth) \| [cfg](detection/configs/cascade_mask_rcnn/cascade_internimage_l_fpn_1x_coco.py) |
| InternImage-L  |     Cascade        |     3x      |  56.1   |   48.5   |  277M   | 1399G |  [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/det_model/cascade_internimage_l_fpn_3x_coco.pth) \| [cfg](detection/configs/cascade_mask_rcnn/cascade_internimage_l_fpn_3x_coco.py) |
| InternImage-XL |     Cascade        |     1x      |  55.3   |   48.1   |  387M   | 1782G |  [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/det_model/cascade_internimage_xl_fpn_1x_coco.pth) \| [cfg](detection/configs/cascade_mask_rcnn/cascade_internimage_xl_fpn_1x_coco.py) |
| InternImage-XL |     Cascade        |     3x      |  56.2   |   48.8   |  387M   | 1782G |  [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/det_model/cascade_internimage_xl_fpn_1x_coco.pth) \| [cfg](detection/configs/cascade_mask_rcnn/cascade_internimage_xl_fpn_3x_coco.py) |
| InternImage-H |     DINO        |     3x      |  65.0   |   -   |  2.18B   | TBD |  TBD |
| InternImage-G |     DINO        |     3x      |  65.3   |   -   |  3B   | TBD |  TBD |

**ADE20Kè¯­ä¹‰åˆ†å‰²**

|    backbone    | resolution | single scale | multi scale | #param | FLOPs | Download | 
| :------------: | :--------: | :----------: | :---------: | :-----: | :---: |   :---:  |
<<<<<<< HEAD
| InternImage-T  |  512x512   |     47.9     |    48.1     |   59M   | 944G  | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/seg_models/upernet_internimage_t_512_160k_ade20k.pth) \| [cfg](segmentation/configs/upernet/upernet_internimage_t_512_160k_ade20k.py) |
| InternImage-S  |  512x512   |     50.1     |    50.9     |   80M   | 1017G | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/seg_models/upernet_internimage_s_512_160k_ade20k.pth) \| [cfg](segmentation/configs/upernet/upernet_internimage_s_512_160k_ade20k.py) |
| InternImage-B  |  512x512   |     50.8     |    51.3     |  128M   | 1185G | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/seg_models/upernet_internimage_b_512_160k_ade20k.pth) \| [cfg](segmentation/configs/upernet/upernet_internimage_b_512_160k_ade20k.py) |
| InternImage-L  |  640x640   |     53.9     |    54.1     |  256M   | 2526G | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/seg_models/upernet_internimage_l_640_160k_ade20k.pth) \| [cfg](segmentation/configs/upernet/upernet_internimage_l_640_160k_ade20k.py) |
| InternImage-XL |  640x640   |     55.0     |    55.3     |  368M   | 3142G | [ckpt](https://github.com/OpenGVLab/InternImage/releases/download/seg_models/upernet_internimage_xl_640_160k_ade20k.pth) \| [cfg](segmentation/configs/upernet/upernet_internimage_xl_640_160k_ade20k.py) |
| InternImage-H |  896x896   |     59.9     |    60.3     |  1.12B   | 3566G | TBD |
| InternImage-H |  896x896   |     62.5     |    62.9     |  1.31B   | 4635G | TBD |


**æ¨¡å‹æ¨ç†é€Ÿåº¦**

|      name      | resolution | #param | FLOPs | Batch 1 FPS(TensorRT) |
| :------------: | :--------: | :-----: | :---: | :-------------------: |
| InternImage-T  |  224x224   |   30M   |  5G   |          156          |
| InternImage-S  |  224x224   |   50M   |  8G   |          129          |
| InternImage-B  |  224x224   |   97M   |  16G  |          116          |
| InternImage-L  |  384x384   |  223M   | 108G  |          56           |
| InternImage-XL |  384x384   |  335M   | 163G  |          47           |


## å¼•ç”¨

è‹¥â€œä¹¦ç”Ÿ2.5â€å¯¹æ‚¨çš„ç ”ç©¶å·¥ä½œæœ‰å¸®åŠ©ï¼Œè¯·å‚è€ƒå¦‚ä¸‹bibtexå¯¹æˆ‘ä»¬çš„å·¥ä½œè¿›è¡Œå¼•ç”¨ã€‚

```
@article{wang2022internimage,
  title={InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions},
  author={Wang, Wenhai and Dai, Jifeng and Chen, Zhe and Huang, Zhenhang and Li, Zhiqi and Zhu, Xizhou and Hu, Xiaowei and Lu, Tong and Lu, Lewei and Li, Hongsheng and others},
  journal={arXiv preprint arXiv:2211.05778},
  year={2022}
}

@inproceedings{zhu2022uni,
  title={Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks},
  author={Zhu, Xizhou and Zhu, Jinguo and Li, Hao and Wu, Xiaoshi and Li, Hongsheng and Wang, Xiaohua and Dai, Jifeng},
  booktitle={CVPR},
  pages={16804--16815},
  year={2022}
}

@article{zhu2022uni,
  title={Uni-perceiver-moe: Learning sparse generalist models with conditional moes},
  author={Zhu, Jinguo and Zhu, Xizhou and Wang, Wenhai and Wang, Xiaohua and Li, Hongsheng and Wang, Xiaogang and Dai, Jifeng},
  journal={arXiv preprint arXiv:2206.04674},
  year={2022}
}

@article{li2022uni,
  title={Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks},
  author={Li, Hao and Zhu, Jinguo and Jiang, Xiaohu and Zhu, Xizhou and Li, Hongsheng and Yuan, Chun and Wang, Xiaohua and Qiao, Yu and Wang, Xiaogang and Wang, Wenhai and others},
  journal={arXiv preprint arXiv:2211.09808},
  year={2022}
}

@article{yang2022bevformer,
  title={BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision},
  author={Yang, Chenyu and Chen, Yuntao and Tian, Hao and Tao, Chenxin and Zhu, Xizhou and Zhang, Zhaoxiang and Huang, Gao and Li, Hongyang and Qiao, Yu and Lu, Lewei and others},
  journal={arXiv preprint arXiv:2211.10439},
  year={2022}
}

@article{su2022towards,
  title={Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information},
  author={Su, Weijie and Zhu, Xizhou and Tao, Chenxin and Lu, Lewei and Li, Bin and Huang, Gao and Qiao, Yu and Wang, Xiaogang and Zhou, Jie and Dai, Jifeng},
  journal={arXiv preprint arXiv:2211.09807},
  year={2022}
}

@inproceedings{li2022bevformer,
  title={Bevformer: Learning birdâ€™s-eye-view representation from multi-camera images via spatiotemporal transformers},
  author={Li, Zhiqi and Wang, Wenhai and Li, Hongyang and Xie, Enze and Sima, Chonghao and Lu, Tong and Qiao, Yu and Dai, Jifeng},
  booktitle={ECCV},
  pages={1--18},
  year={2022},
}
```
